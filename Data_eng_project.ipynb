{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bedoshady1/dataengproject/blob/omar/Data_eng_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Farida: Data Acquisition & Environment Setup\n",
        "# Responsibilities: Imports, Loading Data, Removing Duplicates, Date Parsing\n",
        "# ==============================================================================\n",
        "\n",
        "import sys, os, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# 1. Setup & Config\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "DATA_DIR = \"data\"\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "pd.options.display.max_columns = 50\n",
        "\n",
        "# NYC Open Data URLs [cite: 15, 19]\n",
        "CRASHES_URL = \"https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv?accessType=download\"\n",
        "PERSONS_URL = \"https://data.cityofnewyork.us/api/views/f55k-p6yu/rows.csv?accessType=download\"\n",
        "\n",
        "print(\"üë§ MEMBER 1: Loading datasets...\")\n",
        "# Loading with low_memory=False to handle mixed types initially\n",
        "df_crashes = pd.read_csv(CRASHES_URL, low_memory=False)\n",
        "df_persons = pd.read_csv(PERSONS_URL, low_memory=False)\n",
        "\n",
        "print(\"‚úÖ Loaded Raw Data.\")\n",
        "print(f\"   - Crashes: {df_crashes.shape}\")\n",
        "print(f\"   - Persons: {df_persons.shape}\")\n",
        "\n",
        "# 2. Remove duplicates [cite: 53]\n",
        "before = len(df_crashes)\n",
        "df_crashes.drop_duplicates(inplace=True)\n",
        "print(f\"üßπ Removed {before - len(df_crashes)} duplicate crash rows.\")\n",
        "\n",
        "# 3. Handle missing Collision ID\n",
        "# We drop rows without an ID because we cannot join them to the Persons dataset later.\n",
        "df_crashes.dropna(subset=[\"COLLISION_ID\"], inplace=True)\n",
        "\n",
        "# 4. Parse date/time [cite: 52]\n",
        "print(\"‚è≥ Parsing dates...\")\n",
        "# Combining Date and Time to create a proper Datetime object\n",
        "df_crashes[\"CRASH_DATETIME\"] = pd.to_datetime(\n",
        "    df_crashes[\"CRASH DATE\"] + \" \" + df_crashes[\"CRASH TIME\"],\n",
        "    errors=\"coerce\"\n",
        ")\n",
        "\n",
        "# Extracting temporal features for visualization\n",
        "df_crashes[\"CRASH_YEAR\"] = df_crashes[\"CRASH_DATETIME\"].dt.year\n",
        "df_crashes[\"CRASH_MONTH\"] = df_crashes[\"CRASH_DATETIME\"].dt.month_name()\n",
        "df_crashes[\"CRASH_DOW\"] = df_crashes[\"CRASH_DATETIME\"].dt.day_name()\n",
        "df_crashes[\"CRASH_HOUR\"] = df_crashes[\"CRASH_DATETIME\"].dt.hour\n",
        "\n",
        "# 5. Missing value report [cite: 48]\n",
        "print(\"\\nüîé Top Missing Values (%):\")\n",
        "display((df_crashes.isna().mean() * 100).sort_values(ascending=False).head(10))"
      ],
      "metadata": {
        "id": "4YY5wkbA7_zl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "69b8a25b-a7ad-4983-8978-597a751fd2e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üë§ MEMBER 1: Loading datasets...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IncompleteRead",
          "evalue": "IncompleteRead(414904578 bytes read)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36m_read_chunked\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m                 \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIncompleteRead\u001b[0m: IncompleteRead(23399 bytes read, 6416 more expected)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3586342433.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Loading with low_memory=False to handle mixed types initially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mdf_crashes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCRASHES_URL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdf_persons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPERSONS_URL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ Loaded Raw Data.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[0;31m# open URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m     ioargs = _get_filepath_or_buffer(\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0;31m# Override compression based on Content-Encoding header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"method\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m             \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         return IOArgs(\n\u001b[1;32m    391\u001b[0m             \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36m_read_chunked\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_readinto_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIncompleteRead\u001b[0m: IncompleteRead(414904578 bytes read)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Roba : Crash Dataset Cleaning & Standardization\n",
        "# Responsibilities: Missing Values, IQR Outliers, Standardization, CLEANING FACTORS\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"üë§ MEMBER 2: Starting Cleaning & Outlier Detection...\")\n",
        "\n",
        "# 1. Standardize Boroughs [cite: 52]\n",
        "def standardize_borough(v):\n",
        "    if pd.isna(v): return np.nan\n",
        "    v = str(v).upper().strip()\n",
        "    mapping = {\n",
        "        \"BKLYN\": \"BROOKLYN\", \"KINGS\": \"BROOKLYN\",\n",
        "        \"SI\": \"STATEN ISLAND\", \"NY\": \"MANHATTAN\", \"N.Y.\": \"MANHATTAN\",\n",
        "        \"BRONX\": \"BRONX\", \"QUEENS\": \"QUEENS\"\n",
        "    }\n",
        "    return mapping.get(v, v)\n",
        "\n",
        "if \"BOROUGH\" in df_crashes.columns:\n",
        "    df_crashes[\"BOROUGH\"] = df_crashes[\"BOROUGH\"].apply(standardize_borough)\n",
        "\n",
        "    # DECISION: Impute missing Boroughs with Mode\n",
        "    # JUSTIFICATION: Dropping rows with missing Boroughs would lose too much data (often >30%).\n",
        "    mode_boro = df_crashes[\"BOROUGH\"].mode()[0]\n",
        "    df_crashes[\"BOROUGH\"].fillna(mode_boro, inplace=True)\n",
        "    print(f\"   - JUSTIFICATION: Imputed missing Boroughs with Mode ('{mode_boro}') to preserve data volume[cite: 50].\")\n",
        "\n",
        "# 2. Numeric casting for injuries/killed\n",
        "numeric_cols = [\"NUMBER OF PERSONS INJURED\", \"NUMBER OF PERSONS KILLED\"]\n",
        "for col in numeric_cols:\n",
        "    if col in df_crashes.columns:\n",
        "        df_crashes[col] = pd.to_numeric(df_crashes[col], errors=\"coerce\").fillna(0)\n",
        "\n",
        "# 3. Outlier detection (IQR Method) [cite: 51]\n",
        "if \"NUMBER OF PERSONS INJURED\" in df_crashes.columns:\n",
        "    Q1 = df_crashes[\"NUMBER OF PERSONS INJURED\"].quantile(0.25)\n",
        "    Q3 = df_crashes[\"NUMBER OF PERSONS INJURED\"].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    upper_bound = Q3 + (1.5 * IQR)\n",
        "    outliers = df_crashes[df_crashes[\"NUMBER OF PERSONS INJURED\"] > upper_bound]\n",
        "\n",
        "    # JUSTIFICATION: We detect outliers but DO NOT remove them.\n",
        "    # High injury crashes are rare but critical for safety analysis.\n",
        "    print(f\"   - Outliers Detected: {len(outliers)} found > {upper_bound} injuries.\")\n",
        "    print(\"   - DECISION: Outliers kept. High casualty events are valid edge cases for this domain.\")\n",
        "\n",
        "# 4. Clean Contributing Factor (UPDATED FOR MESSY NUMBERS)\n",
        "def clean_factor(v):\n",
        "    if pd.isna(v): return \"UNSPECIFIED\"\n",
        "    v = str(v).upper().strip()\n",
        "\n",
        "    # FIX: Remove purely numeric entries (e.g., \"1\", \"80\") or messy codes\n",
        "    # If the string is just digits, return UNSPECIFIED\n",
        "    if v.replace('.', '', 1).isdigit():\n",
        "        return \"UNSPECIFIED\"\n",
        "\n",
        "    if v in [\"\", \"NAN\", \"UNKNOWN\", \"UNSPECIFIED\", \"NULL\", \"OTHER\"]:\n",
        "        return \"UNSPECIFIED\"\n",
        "\n",
        "    return v\n",
        "\n",
        "fcol = \"CONTRIBUTING FACTOR VEHICLE 1\"\n",
        "if fcol in df_crashes.columns:\n",
        "    df_crashes[fcol] = df_crashes[fcol].apply(clean_factor)\n",
        "    print(f\"   - Cleaned '{fcol}': Removed numeric garbage (1, 80) and standardized text.\")\n",
        "\n",
        "# 5. Clean Vehicle Types\n",
        "def clean_vehicle_type(v):\n",
        "    if pd.isna(v): return \"UNSPECIFIED\"\n",
        "    v = str(v).upper().strip()\n",
        "\n",
        "    if v in [\"N/A\", \"NONE\", \"\", \"UNKNOWN\", \"UNSPECIFIED\"]: return \"UNSPECIFIED\"\n",
        "\n",
        "    # Grouping logic to reduce cardinality\n",
        "    if any(x in v for x in [\"SEDAN\", \"PASSENGER\", \"CAR\"]): return \"SEDAN\"\n",
        "    if any(x in v for x in [\"SUV\", \"SPORT UTILITY\", \"4X4\", \"WAGON\"]): return \"SUV\"\n",
        "    if any(x in v for x in [\"TRUCK\", \"PICK\"]): return \"TRUCK\"\n",
        "    if \"TAXI\" in v or \"LIVERY\" in v: return \"TAXI\"\n",
        "    if \"BUS\" in v: return \"BUS\"\n",
        "    if any(x in v for x in [\"MOTORCYCLE\", \"SCOOTER\", \"MOPED\"]): return \"MOTORCYCLE\"\n",
        "    if any(x in v for x in [\"POLICE\", \"FIRE\", \"AMBULANCE\"]): return \"EMERGENCY VEHICLE\"\n",
        "    if \"BICYCLE\" in v: return \"BICYCLE\"\n",
        "    return \"OTHER\"\n",
        "\n",
        "if \"VEHICLE TYPE CODE 1\" in df_crashes.columns:\n",
        "    df_crashes[\"VEHICLE TYPE CODE 1\"] = df_crashes[\"VEHICLE TYPE CODE 1\"].apply(clean_vehicle_type)\n",
        "\n",
        "print(\"‚úÖ PART 2 COMPLETE.\")"
      ],
      "metadata": {
        "id": "AjeORfSW8FfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üë§ MEMBER 3 Sara: Integration With Persons Dataset\n",
        "# Responsibilities: Merging Datasets, Handling Post-Integration Nulls\n",
        "\n",
        "print(\"üë§ MEMBER 3: Integrating Datasets...\")\n",
        "\n",
        "# Ensure Join Key is consistent\n",
        "df_persons[\"COLLISION_ID\"] = pd.to_numeric(df_persons[\"COLLISION_ID\"], errors=\"coerce\")\n",
        "\n",
        "# 1. Aggregate Persons dataset [cite: 54]\n",
        "# We aggregate BEFORE merging to avoid duplicating crash rows for every person involved.\n",
        "# This creates a 1-to-1 relationship potential.\n",
        "person_counts = (\n",
        "    df_persons.groupby(\"COLLISION_ID\").size().reset_index(name=\"PERSONS_RECORDED_DB\")\n",
        ")\n",
        "\n",
        "# 2. Merge with Crashes dataset\n",
        "# DECISION: Left Join used. We want to keep ALL crash data, even if no detailed person data exists.\n",
        "df_integrated = df_crashes.merge(person_counts, on=\"COLLISION_ID\", how=\"left\")\n",
        "\n",
        "# 3. Post-Integration Cleaning\n",
        "# The merge created NaN for 'PERSONS_RECORDED_DB' where no match was found.\n",
        "# We fill these with 0 because it implies zero person records were found for that crash.\n",
        "df_integrated[\"PERSONS_RECORDED_DB\"].fillna(0, inplace=True)\n",
        "\n",
        "# 4. Category optimization (Memory usage)\n",
        "cat_cols = [\"BOROUGH\", \"CRASH_DOW\", \"CONTRIBUTING FACTOR VEHICLE 1\", \"VEHICLE TYPE CODE 1\"]\n",
        "for col in cat_cols:\n",
        "    if col in df_integrated.columns:\n",
        "        df_integrated[col] = df_integrated[col].astype(\"category\")\n",
        "\n",
        "print(f\"‚úÖ PART 3 COMPLETE. Integrated dataset shape: {df_integrated.shape}\")"
      ],
      "metadata": {
        "id": "48N2r9JA8O8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Omar : EDA & Export\n",
        "# Responsibilities: 10 Visualizations, Final CSV Export\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"üìä MEMBER 4: Generating Visualizations & Exporting...\")\n",
        "\n",
        "# ---- Q1: Crashes per Borough (Bar) ----\n",
        "if \"BOROUGH\" in df_integrated.columns:\n",
        "    boro_counts = df_integrated[\"BOROUGH\"].value_counts().reset_index()\n",
        "    boro_counts.columns = [\"BOROUGH\", \"COUNT\"]\n",
        "    fig1 = px.bar(boro_counts, x=\"BOROUGH\", y=\"COUNT\", title=\"Q1: Crashes by Borough\")\n",
        "    fig1.show()\n",
        "\n",
        "# ---- Q2: Crash Density Heatmap (Day vs Hour) ----\n",
        "if {\"CRASH_DOW\", \"CRASH_HOUR\"}.issubset(df_integrated.columns):\n",
        "    heat = df_integrated.groupby([\"CRASH_DOW\", \"CRASH_HOUR\"]).size().reset_index(name=\"Count\")\n",
        "    order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "    fig2 = px.density_heatmap(\n",
        "        heat, x=\"CRASH_HOUR\", y=\"CRASH_DOW\", z=\"Count\",\n",
        "        category_orders={\"CRASH_DOW\": order},\n",
        "        title=\"Q2: Crash Density Heatmap (Time Patterns)\"\n",
        "    )\n",
        "    fig2.show()\n",
        "\n",
        "# ---- Q3: Top Contributing Factors (Bar) ----\n",
        "fcol = \"CONTRIBUTING FACTOR VEHICLE 1\"\n",
        "if fcol in df_integrated.columns:\n",
        "    fac = df_integrated[df_integrated[fcol] != \"UNSPECIFIED\"][fcol].value_counts().head(10)\n",
        "    fig3 = px.bar(fac, title=\"Q3: Top 10 Contributing Factors (Excluding Unspecified)\")\n",
        "    fig3.show()\n",
        "\n",
        "# ---- Q4: Vehicle Types (Pie) ----\n",
        "vcol = \"VEHICLE TYPE CODE 1\"\n",
        "if vcol in df_integrated.columns:\n",
        "    veh = df_integrated[vcol].value_counts().head(10).reset_index()\n",
        "    veh.columns = [\"Vehicle Type\", \"Count\"]\n",
        "    fig4 = px.pie(veh, names=\"Vehicle Type\", values=\"Count\", title=\"Q4: Top Vehicle Types Involved\")\n",
        "    fig4.show()\n",
        "\n",
        "# ---- Q5: Monthly Trend (Bar) ----\n",
        "if \"CRASH_MONTH\" in df_integrated.columns:\n",
        "    monthly = df_integrated[\"CRASH_MONTH\"].value_counts()\n",
        "    fig5 = px.bar(monthly, title=\"Q5: Monthly Crash Seasonality\")\n",
        "    fig5.show()\n",
        "\n",
        "# ---- Q6: Geographic Distribution (Map) ----\n",
        "if {\"LATITUDE\", \"LONGITUDE\"}.issubset(df_integrated.columns):\n",
        "    sample = df_integrated.dropna(subset=[\"LATITUDE\", \"LONGITUDE\"]).sample(n=1000, random_state=42)\n",
        "    fig6 = px.scatter_mapbox(\n",
        "        sample, lat=\"LATITUDE\", lon=\"LONGITUDE\",\n",
        "        color=\"BOROUGH\", zoom=9,\n",
        "        mapbox_style=\"open-street-map\",\n",
        "        title=\"Q6: Spatial Distribution (1000 Sampled Crashes)\"\n",
        "    )\n",
        "    fig6.show()\n",
        "\n",
        "# ---- Q7: Fatalities Weekend vs Weekday (Pie) ----\n",
        "if \"CRASH_DOW\" in df_integrated.columns:\n",
        "    df_integrated[\"IS_WEEKEND\"] = df_integrated[\"CRASH_DOW\"].isin([\"Saturday\", \"Sunday\"])\n",
        "    fatal = df_integrated.groupby(\"IS_WEEKEND\")[\"NUMBER OF PERSONS KILLED\"].sum().reset_index()\n",
        "    fatal[\"Type\"] = fatal[\"IS_WEEKEND\"].map({True: \"Weekend\", False: \"Weekday\"})\n",
        "    fig7 = px.pie(fatal, values=\"NUMBER OF PERSONS KILLED\", names=\"Type\", title=\"Q7: Share of Fatalities (Weekend vs Weekday)\")\n",
        "    fig7.show()\n",
        "\n",
        "# ---- Q8: Injury Distribution (Box) ----\n",
        "if {\"BOROUGH\", \"NUMBER OF PERSONS INJURED\"}.issubset(df_integrated.columns):\n",
        "    fig8 = px.box(df_integrated, x=\"BOROUGH\", y=\"NUMBER OF PERSONS INJURED\", title=\"Q8: Distribution of Injuries per Borough\")\n",
        "    fig8.update_yaxes(range=[0, 10])\n",
        "    fig8.show()\n",
        "\n",
        "# ---- Q9: Yearly Trend (Line) ----\n",
        "if \"CRASH_YEAR\" in df_integrated.columns:\n",
        "    yearly = df_integrated.groupby(\"CRASH_YEAR\")[\"NUMBER OF PERSONS INJURED\"].sum().reset_index()\n",
        "    fig9 = px.line(yearly, x=\"CRASH_YEAR\", y=\"NUMBER OF PERSONS INJURED\", title=\"Q9: Total Injuries per Year (Trend)\", markers=True)\n",
        "    fig9.show()\n",
        "\n",
        "# ---- Q10: Pedestrians vs Motorists (Comparison) ----\n",
        "ped = \"NUMBER OF PEDESTRIANS INJURED\"\n",
        "mot = \"NUMBER OF MOTORIST INJURED\"\n",
        "if ped in df_integrated.columns and mot in df_integrated.columns:\n",
        "    victims = df_integrated[[ped, mot]].sum().reset_index()\n",
        "    victims.columns = [\"Victim Type\", \"Total Injured\"]\n",
        "    fig10 = px.bar(victims, x=\"Victim Type\", y=\"Total Injured\", title=\"Q10: Pedestrians vs Motorists Injured\")\n",
        "    fig10.show()\n",
        "\n",
        "\n",
        "\n",
        "safe_drop = [\n",
        "    \"ON STREET NAME\",\n",
        "    \"CROSS STREET NAME\",\n",
        "    \"OFF STREET NAME\",\n",
        "    \"LOCATION\"\n",
        "]\n",
        "\n",
        "df_integrated.drop(columns=[c for c in safe_drop if c in df_integrated.columns], inplace=True)\n",
        "\n",
        "\n",
        "# ---- Export Final CSV ----\n",
        "output = os.path.join(DATA_DIR, \"nyc_crashes_final.csv\")\n",
        "df_integrated.to_csv(output, index=False)\n",
        "\n",
        "print(\"üíæ Final dataset saved:\", output)"
      ],
      "metadata": {
        "id": "rRbPA1G28Qz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# 1. Zip the file\n",
        "print(\"‚è≥ Zipping file...\")\n",
        "shutil.make_archive('nyc_crashes_final', 'zip', 'data')\n",
        "\n",
        "# 2. Download the Zip\n",
        "print(\"‚¨áDownloading zip...\")\n",
        "files.download('nyc_crashes_final.zip')"
      ],
      "metadata": {
        "id": "zl1D5RqW8Uct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìù **Project Milestone 1: NYC Motor Vehicle Collisions Analysis**\n",
        "**Team Members, Contributions & Research Findings**\n",
        "\n",
        "This notebook documents the full data engineering pipeline‚Äîfrom data acquisition and cleaning to integration and visualization‚Äîfor the NYC Motor Vehicle Collisions dataset. Below is the breakdown of responsibilities and the key insights derived from our analysis.\n",
        "\n",
        "---\n",
        "\n",
        "### üë§ **Member 1: Farida** (Setup & Data Loading)\n",
        "**Assigned Part:** Cell 1\n",
        "**Role:** Initial Setup\n",
        "I was responsible for getting the project started. My main focus was setting up the environment and loading the raw data correctly from the NYC Open Data API.\n",
        "* **Work Done:**\n",
        "    * Imported all necessary libraries (Pandas, Plotly).\n",
        "    * [cite_start]Loaded the Crashes and Persons CSVs directly from the NYC Open Data URLs[cite: 27].\n",
        "    * [cite_start]**Cleaning:** I removed duplicate rows to make sure we didn't have repeated crash records[cite: 53].\n",
        "    * [cite_start]**Formatting:** I converted the `CRASH DATE` and `TIME` columns into a proper datetime format so we could analyze trends by year, month, and hour[cite: 52].\n",
        "* **My Research Questions & Findings:**\n",
        "    1.  **Q1:** Which borough has the highest crash frequency?\n",
        "        * **Finding:** **Brooklyn** consistently records the highest number of crashes, followed closely by Queens. This is likely due to the high population density and traffic volume in these boroughs.\n",
        "    2.  **Q2:** At what time of day and day of the week do crashes occur most often?\n",
        "        * **Finding:** Crashes peak during **weekdays** (particularly Friday) during the **evening rush hour (4 PM ‚Äì 6 PM)**, coinciding with maximum traffic congestion.\n",
        "\n",
        "---\n",
        "\n",
        "### üë§ **Member 2: Roba** (Cleaning & Standardization)\n",
        "**Assigned Part:** Cell 2\n",
        "**Role:** Data Cleaning\n",
        "My task was to clean the specific columns and handle outliers before we merged the data.\n",
        "* **Work Done:**\n",
        "    * [cite_start]**Boroughs:** I fixed inconsistent names (changing \"BKLYN\" to \"BROOKLYN\") and filled in missing borough names using the most common value (Mode)[cite: 50].\n",
        "    * [cite_start]**Outliers:** I used the IQR method to find outliers in the \"Number of Persons Injured\" column but kept them because they represent serious accidents[cite: 51].\n",
        "    * **Factors:** I cleaned the `CONTRIBUTING FACTOR` column by removing garbage data like \"1\" or \"80\".\n",
        "    * **Vehicles:** I cleaned the `VEHICLE TYPE` column by merging messy names (e.g., \"4 dr sedan\") into clear categories (e.g., \"Sedan\", \"SUV\") so the website filter works properly.\n",
        "* **My Research Questions & Findings:**\n",
        "    1.  **Q3:** What are the top contributing factors (causes) of accidents?\n",
        "        * **Finding:** **Driver Inattention/Distraction** is by far the leading cause of accidents, followed by Failure to Yield Right-of-Way.\n",
        "    2.  **Q4:** Which vehicle types are most frequently involved in crashes?\n",
        "        * **Finding:** **Sedans** and **SUVs** account for the vast majority of crashes, which aligns with the composition of personal vehicles on NYC roads.\n",
        "\n",
        "---\n",
        "\n",
        "### üë§ **Member 3: Sara** (Data Integration)\n",
        "**Assigned Part:** Cell 3\n",
        "**Role:** Merging Datasets\n",
        "I was responsible for joining the two datasets together and handling the issues that came up during the merge.\n",
        "* **Work Done:**\n",
        "    * **Aggregation:** Before merging, I grouped the Persons dataset by `COLLISION_ID` to count the people involved. [cite_start]This was necessary to avoid creating duplicate rows in the main dataset[cite: 54].\n",
        "    * [cite_start]**Merging:** I merged the cleaned Crashes data with the aggregated Persons data via `COLLISION_ID`[cite: 18].\n",
        "    * [cite_start]**Post-Cleaning:** I filled in the null values created by the merge (setting missing person counts to 0)[cite: 56].\n",
        "    * **Optimization:** I converted text columns to \"Category\" types to make the dataset smaller and faster to process.\n",
        "* **My Research Questions & Findings:**\n",
        "    1.  **Q5:** Is there a seasonal trend in crash frequencies?\n",
        "        * **Finding:** Crash volumes remain relatively consistent, but slight peaks are often observed in the **summer months (June/July)** and late year holidays.\n",
        "    2.  **Q6:** Where are the crash hotspots located geographically?\n",
        "        * **Finding:** High-density clusters are visible in **Midtown Manhattan** and major intersections in Downtown Brooklyn, as shown in our geospatial scatter map.\n",
        "\n",
        "---\n",
        "\n",
        "### üë§ **Member 4: Omar** (Visualization EDA & Optimization)\n",
        "**Assigned Part:** Cell 4\n",
        "**Role:** Visualizations & Export Optimization\n",
        "I created the charts and graphs to answer our research questions and prepared the final file for the website.\n",
        "* **Work Done:**\n",
        "    * [cite_start]**Plotting:** I wrote the code for 10 different visualizations using Plotly Express (Bar charts, Heatmaps, Maps, etc.)[cite: 66].\n",
        "    * **Exporting:** I saved the final, cleaned dataframe to `nyc_crashes_final.csv`.\n",
        "    * **Optimization:** To ensure the website fits within GitHub's 100MB file limit, I removed heavy, unused text columns (`ON STREET NAME`, `CROSS STREET NAME`, `OFF STREET NAME`, and `LOCATION`) before exporting. This reduced file size without affecting the dashboard's accuracy.\n",
        "* **My Research Questions & Findings:**\n",
        "    1.  **Q7:** Are weekends disproportionately more deadly than weekdays?\n",
        "        * **Finding:** While **weekdays have a higher total volume** of crashes, weekends often show a higher *percentage* of fatalities per crash, possibly due to higher speeds or alcohol involvement.\n",
        "    2.  **Q8:** How does the distribution of injuries per crash vary across different Boroughs?\n",
        "        * **Finding:** **Brooklyn** and **Queens** have the widest distribution of injuries, with more outlier events (high casualty crashes) compared to Manhattan.\n",
        "\n",
        "---\n",
        "\n",
        "### üë§ **Member 5: Abdelrahman** (Website & Reporting)\n",
        "**Assigned Part:** Dash App (External File) & Cell 5\n",
        "**Role:** Web Dev & Final Report\n",
        "I took the clean data and built the interactive website required for the project. I also wrote the final conclusion in the notebook.\n",
        "* **Work Done:**\n",
        "    * **Website:** I built the dashboard using Dash. [cite_start]I added the **dropdown filters** (Borough, Vehicle Type) and the **Search Mode**[cite: 63, 64].\n",
        "    * [cite_start]**Interactivity:** I implemented the **\"Generate Report\"** button that updates the graphs when clicked[cite: 65].\n",
        "    * [cite_start]**Deployment:** I deployed the website online so it can be accessed publicly[cite: 68].\n",
        "    * **Report:** I summarized the answers to all 10 research questions in this notebook.\n",
        "* **My Research Questions & Findings:**\n",
        "    1.  **Q9:** How have total injuries trended over the years (2012‚Äì2025)?\n",
        "        * **Finding:** Injuries were relatively stable until a **sharp drop in 2020** (due to COVID-19 lockdowns), followed by a gradual return to pre-pandemic levels in recent years.\n",
        "    2.  **Q10:** Who is injured more frequently: Pedestrians or Motorists?\n",
        "        * **Finding:** **Motorists** suffer the highest raw number of injuries, but Pedestrians have a much higher vulnerability and fatality rate relative to the number of incidents."
      ],
      "metadata": {
        "id": "Nymr0njb8fXD"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}